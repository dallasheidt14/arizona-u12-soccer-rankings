name: Daily Soccer Data Scrape

on:
  schedule:
    - cron: "30 10 * * *"  # 10:30 UTC = 03:30 Phoenix (no DST)
  workflow_dispatch:  # Manual trigger
    inputs:
      force_run:
        description: 'Force run even if recent run exists'
        required: false
        default: 'false'
        type: boolean

jobs:
  scrape:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
      
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"
      
      - name: Install dependencies
        run: |
          python -m pip install -U pip
          pip install httpx rapidfuzz pandas pyarrow
      
      - name: Run daily scraper
        run: |
          python scraper_daily.py
        env:
          # Add any environment variables needed for scraping
          SCRAPER_ENV: production
      
      - name: Upload scrape artifacts
        uses: actions/upload-artifact@v4
        with:
          name: scrape-outputs-${{ github.run_number }}
          path: |
            data_ingest/**
            scraper.log
          retention-days: 7
      
      - name: Commit updated data (if any)
        if: github.event_name == 'schedule'  # Only auto-commit on scheduled runs
        run: |
          git config --local user.email "action@github.com"
          git config --local user.name "GitHub Action"
          
          # Check if there are changes to commit
          if git diff --quiet data_ingest/; then
            echo "No changes to commit"
          else
            git add data_ingest/
            git commit -m "Daily scrape update - $(date)"
            git push
          fi
      
      - name: Upload logs on failure
        if: failure()
        uses: actions/upload-artifact@v4
        with:
          name: scraper-error-logs-${{ github.run_number }}
          path: |
            scraper.log
            data_ingest/
          retention-days: 30
