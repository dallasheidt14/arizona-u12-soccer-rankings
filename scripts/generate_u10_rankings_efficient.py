#!/usr/bin/env python3
"""
Efficient U10 Rankings Generator with Alias System
=================================================

This script generates U10 rankings with sophisticated team matching,
using a persistent alias system for O(1) lookups.

Key improvements:
- Persistent CSV-based alias table (saves hours of processing)
- O(1) lookups for known teams, fallback to sophisticated matching for unknowns
- Much faster subsequent runs (~5 min vs ~60 min)
"""

import pandas as pd
import numpy as np
import os
import sys
from datetime import datetime, timedelta
import warnings
import time
warnings.filterwarnings('ignore')

# Add project root to path
sys.path.append(os.path.dirname(os.path.dirname(__file__)))

# Import sophisticated team matcher and alias resolver
from sophisticated_team_matcher import SophisticatedTeamMatcher
from src.identity.alias_resolver import AliasResolver

# Phase 4 Step 4: Import DuckDB for fast aggregations
try:
    import duckdb
    DUCKDB_AVAILABLE = True
    print("‚úÖ DuckDB available for fast aggregations")
except ImportError:
    DUCKDB_AVAILABLE = False
    print("‚ö†Ô∏è DuckDB not available, using pandas fallback")

# Phase 4 Step 5: Import joblib for parallelization
try:
    from joblib import Parallel, delayed
    JOBLIB_AVAILABLE = True
    print("‚úÖ Joblib available for parallelization")
except ImportError:
    JOBLIB_AVAILABLE = False
    print("‚ö†Ô∏è Joblib not available, using sequential processing")

class EfficientU10RankingsGenerator:
    """Generate U10 rankings with alias-based sophisticated team matching"""
    
    def __init__(self):
        self.u10_master_df = None
        self.u11_master_df = None
        self.u10_games_df = None
        self.matcher = SophisticatedTeamMatcher()
        self.resolver = None
        
    def load_data(self):
        """Load all required data files"""
        print("=== LOADING DATA ===")
        
        # Load U10 master team list
        u10_master_path = 'data/input/National_Male_U10_Master_Team_List.csv'
        if os.path.exists(u10_master_path):
            self.u10_master_df = pd.read_csv(u10_master_path)
            print(f"‚úÖ Loaded U10 master team list: {len(self.u10_master_df)} teams")
        else:
            print(f"‚ùå U10 master team list not found: {u10_master_path}")
            return False
        
        # Load U11 master team list (for cross-age games)
        u11_master_path = 'data/input/National_Male_U11_Master_Team_List.csv'
        if os.path.exists(u11_master_path):
            self.u11_master_df = pd.read_csv(u11_master_path)
            print(f"‚úÖ Loaded U11 master team list: {len(self.u11_master_df)} teams")
        else:
            print(f"‚ùå U11 master team list not found: {u11_master_path}")
            return False
        
        # Load U10 game history
        games_path = 'data/Game History u10 and u11.csv'
        if os.path.exists(games_path):
            self.u10_games_df = pd.read_csv(games_path)
            # Clean column names
            self.u10_games_df.columns = self.u10_games_df.columns.str.strip()
            print(f"‚úÖ Loaded game history: {len(self.u10_games_df)} games")
        else:
            print(f"‚ùå Game history not found: {games_path}")
            return False
        
        # Initialize alias resolver after loading master lists
        print("üîÑ Initializing alias resolver...")
        
        # Combine U10 and U11 master lists for comprehensive matching
        combined_master_df = pd.concat([
            self.u10_master_df,
            self.u11_master_df
        ], ignore_index=True)
        
        self.resolver = AliasResolver(
            master_df=combined_master_df,
            matcher=self.matcher,
            alias_csv_path='data/mappings/team_alias_table.csv',
            threshold=0.82,
            dry_run=False
        )
        
        stats = self.resolver.get_stats()
        print(f"‚úÖ Alias resolver initialized: {stats['cached_aliases']} cached aliases")
        
        return True
    
    def build_team_game_histories_with_matching(self):
        """Build comprehensive game histories with alias-based sophisticated matching"""
        print("\n=== BUILDING TEAM GAME HISTORIES WITH ALIAS MATCHING ===")
        
        # Phase 4 Step 3: Vectorized alias lookups
        alias_start_time = time.time()
        
        # Get alias dictionary for vectorized operations
        alias_dict = self.resolver.get_alias_dict()
        print(f"üìä Using {len(alias_dict)} cached aliases for vectorized lookup")
        
        # Create a copy of games dataframe for processing
        games_df = self.u10_games_df.copy()
        
        # Vectorized alias resolution using .replace()
        print("üîÑ Applying vectorized alias resolution...")
        games_df['Team A Canonical'] = games_df['Team A'].replace(alias_dict)
        games_df['Team B Canonical'] = games_df['Team B'].replace(alias_dict)
        
        # Find unmatched teams for batch processing
        unmatched_a = games_df.loc[games_df['Team A Canonical'] == games_df['Team A'], 'Team A'].unique()
        unmatched_b = games_df.loc[games_df['Team B Canonical'] == games_df['Team B'], 'Team B'].unique()
        unmatched_teams = list(set(unmatched_a) | set(unmatched_b))
        
        print(f"üîç Found {len(unmatched_teams)} unmatched teams, batch resolving...")
        
        # Batch resolve unmatched teams
        if unmatched_teams:
            batch_results = self.resolver.batch_resolve(unmatched_teams)
            # Update the dataframe with batch results
            games_df['Team A Canonical'] = games_df['Team A Canonical'].replace(batch_results)
            games_df['Team B Canonical'] = games_df['Team B Canonical'].replace(batch_results)
        
        print(f"‚è± Vectorized alias resolution took: {time.time() - alias_start_time:.1f}s")
        
        # Get U10 teams from master list that have games
        u10_teams = set(self.u10_master_df['Team_Name'].unique())
        teams_with_games = set(games_df['Team A Canonical'].unique()) | set(games_df['Team B Canonical'].unique())
        teams_to_rank = u10_teams & teams_with_games
        
        print(f"U10 teams in master list: {len(u10_teams)}")
        print(f"Teams with U10 games: {len(teams_with_games)}")
        print(f"U10 teams to rank: {len(teams_to_rank)}")
        
        # Build histories with canonical team names
        histories = []
        
        for team in teams_to_rank:
            # Find all game history entries for this canonical team
            team_game_entries = []
            
            # Get games where this team is Team A
            team_a_games = games_df[games_df['Team A Canonical'] == team].copy()
            if len(team_a_games) > 0:
                team_a_games['Team'] = team
                team_a_games['Opponent'] = team_a_games['Team B Canonical']
                team_game_entries.append(team_a_games)
            
            # Get games where this team is Team B
            team_b_games = games_df[games_df['Team B Canonical'] == team].copy()
            if len(team_b_games) > 0:
                team_b_games['Team'] = team
                team_b_games['Opponent'] = team_b_games['Team A Canonical']
                team_game_entries.append(team_b_games)
            
            if team_game_entries:
                # Combine all game entries for this team
                combined_games = pd.concat(team_game_entries, ignore_index=True)
                
                # Remove duplicates
                combined_games = combined_games.drop_duplicates(subset=['Date', 'Team', 'Opponent'])
                
                # Build team history
                history = self._build_single_team_history(team, combined_games)
                if history:
                    histories.append(history)
        
        print(f"Built histories for {len(histories)} teams")
        return histories
    
    def _build_single_team_history(self, team, games_df):
        """Build history for a single team"""
        if len(games_df) == 0:
            return None
        
        # Sort by date (most recent first)
        games_df = games_df.sort_values('Date', ascending=False)
        
        # Take most recent 30 games
        recent_games = games_df.head(30)
        
        # Calculate basic stats
        wins = len(recent_games[recent_games['Team A Score'] > recent_games['Team B Score']])
        losses = len(recent_games[recent_games['Team A Score'] < recent_games['Team B Score']])
        ties = len(recent_games[recent_games['Team A Score'] == recent_games['Team B Score']])
        
        total_games = len(games_df)  # Total games in history
        games_played = len(recent_games)  # Games used for ranking
        
        # Calculate goals
        goals_for = recent_games['Team A Score'].sum()
        goals_against = recent_games['Team B Score'].sum()
        
        return {
            'Team': team,
            'Games_Played': games_played,
            'Total_Games': total_games,
            'Wins': wins,
            'Losses': losses,
            'Ties': ties,
            'Goals_For': goals_for,
            'Goals_Against': goals_against,
            'Goal_Diff': goals_for - goals_against,
            'Win_Pct': wins / games_played if games_played > 0 else 0,
            'Games': recent_games
        }
    
    def _aggregate_team_stats_with_duckdb(self, games_df):
        """
        Use DuckDB for fast aggregations on large datasets.
        
        Args:
            games_df: DataFrame with game data
            
        Returns:
            DataFrame with team statistics
        """
        if not DUCKDB_AVAILABLE:
            # Fallback to pandas
            return self._aggregate_team_stats_pandas(games_df)
        
        try:
            # Create DuckDB connection
            con = duckdb.connect()
            
            # Register the dataframe
            con.register('games', games_df)
            
            # Fast aggregation query
            query = """
            SELECT 
                team,
                COUNT(*) as games_played,
                SUM(CASE WHEN goals_for > goals_against THEN 1 ELSE 0 END) as wins,
                SUM(CASE WHEN goals_for < goals_against THEN 1 ELSE 0 END) as losses,
                SUM(CASE WHEN goals_for = goals_against THEN 1 ELSE 0 END) as ties,
                SUM(goals_for) as goals_for,
                SUM(goals_against) as goals_against,
                SUM(goals_for) - SUM(goals_against) as goal_diff,
                AVG(CASE WHEN goals_for > goals_against THEN 1.0 
                         WHEN goals_for < goals_against THEN 0.0 
                         ELSE 0.5 END) as win_pct
            FROM games
            GROUP BY team
            ORDER BY win_pct DESC, goal_diff DESC
            """
            
            team_stats_df = con.execute(query).df()
            con.close()
            
            return team_stats_df
            
        except Exception as e:
            print(f"‚ö†Ô∏è DuckDB aggregation failed: {e}, falling back to pandas")
            return self._aggregate_team_stats_pandas(games_df)
    
    def _aggregate_team_stats_pandas(self, games_df):
        """Fallback pandas aggregation method."""
        team_stats = []
        
        for team in games_df['team'].unique():
            team_games = games_df[games_df['team'] == team]
            
            wins = len(team_games[team_games['goals_for'] > team_games['goals_against']])
            losses = len(team_games[team_games['goals_for'] < team_games['goals_against']])
            ties = len(team_games[team_games['goals_for'] == team_games['goals_against']])
            
            goals_for = team_games['goals_for'].sum()
            goals_against = team_games['goals_against'].sum()
            
            team_stats.append({
                'team': team,
                'games_played': len(team_games),
                'wins': wins,
                'losses': losses,
                'ties': ties,
                'goals_for': goals_for,
                'goals_against': goals_against,
                'goal_diff': goals_for - goals_against,
                'win_pct': wins / len(team_games) if len(team_games) > 0 else 0
            })
        
        return pd.DataFrame(team_stats)
    
    def _calculate_sos_parallel(self, team_histories, team_strengths):
        """
        Calculate Strength of Schedule using parallel processing.
        
        Args:
            team_histories: List of team history dictionaries
            team_strengths: Dictionary of team strengths
            
        Returns:
            Dictionary of team SOS values
        """
        if not JOBLIB_AVAILABLE:
            # Fallback to sequential processing
            return self._calculate_sos_sequential(team_histories, team_strengths)
        
        def calculate_team_sos(team_history):
            """Calculate SOS for a single team."""
            team = team_history['Team']
            games = team_history['Games']
            
            if len(games) == 0:
                return team, 0.0
            
            # Calculate average opponent strength
            opponent_strengths = []
            for _, game in games.iterrows():
                opponent = game['Opponent']
                if opponent in team_strengths:
                    opponent_strengths.append(team_strengths[opponent])
                else:
                    # Default strength for unknown opponents
                    opponent_strengths.append(0.5)
            
            sos = sum(opponent_strengths) / len(opponent_strengths) if opponent_strengths else 0.0
            return team, sos
        
        # Parallel processing
        print(f"üîÑ Calculating SOS for {len(team_histories)} teams in parallel...")
        sos_results = Parallel(n_jobs=-1, backend='threading')(
            delayed(calculate_team_sos)(team_history) for team_history in team_histories
        )
        
        # Convert to dictionary
        sos_dict = dict(sos_results)
        return sos_dict
    
    def _calculate_sos_sequential(self, team_histories, team_strengths):
        """Fallback sequential SOS calculation."""
        sos_dict = {}
        
        for team_history in team_histories:
            team = team_history['Team']
            games = team_history['Games']
            
            if len(games) == 0:
                sos_dict[team] = 0.0
                continue
            
            # Calculate average opponent strength
            opponent_strengths = []
            for _, game in games.iterrows():
                opponent = game['Opponent']
                if opponent in team_strengths:
                    opponent_strengths.append(team_strengths[opponent])
                else:
                    # Default strength for unknown opponents
                    opponent_strengths.append(0.5)
            
            sos_dict[team] = sum(opponent_strengths) / len(opponent_strengths) if opponent_strengths else 0.0
        
        return sos_dict
    
    def _save_cache_parquet(self, data, filename):
        """
        Save data to parquet format for fast loading.
        
        Args:
            data: DataFrame or dictionary to save
            filename: Output filename
        """
        try:
            cache_dir = 'data/cache'
            os.makedirs(cache_dir, exist_ok=True)
            
            filepath = os.path.join(cache_dir, filename)
            
            if isinstance(data, pd.DataFrame):
                data.to_parquet(filepath, index=False)
            else:
                # Convert dict to DataFrame if needed
                df = pd.DataFrame(list(data.items()), columns=['key', 'value'])
                df.to_parquet(filepath, index=False)
            
            print(f"üíæ Cached {filename} to parquet format")
            
        except Exception as e:
            print(f"‚ö†Ô∏è Failed to save parquet cache {filename}: {e}")
    
    def _load_cache_parquet(self, filename):
        """
        Load data from parquet format.
        
        Args:
            filename: Input filename
            
        Returns:
            DataFrame or None if not found
        """
        try:
            cache_dir = 'data/cache'
            filepath = os.path.join(cache_dir, filename)
            
            if os.path.exists(filepath):
                df = pd.read_parquet(filepath)
                print(f"üìÇ Loaded {filename} from parquet cache")
                return df
            else:
                return None
                
        except Exception as e:
            print(f"‚ö†Ô∏è Failed to load parquet cache {filename}: {e}")
            return None
    
    def generate_rankings(self):
        """Generate U10 rankings with alias-based sophisticated matching"""
        print("\n=== GENERATING U10 RANKINGS WITH ALIAS MATCHING ===")
        
        # Phase 4 Step 1: Add timing diagnostics
        total_start_time = time.time()
        
        # Load data (includes resolver initialization)
        t0 = time.time()
        if not self.load_data():
            return None
        print(f"‚è± Data Loading took: {time.time() - t0:.1f}s")
        
        # Build team histories (uses resolver for alias matching)
        t1 = time.time()
        
        # Phase 4 Step 7: Batch I/O operations and integrate all optimizations
        print("üöÄ Phase 4 Performance Optimizations Active:")
        print(f"   ‚Ä¢ DuckDB: {'‚úÖ' if DUCKDB_AVAILABLE else '‚ùå'}")
        print(f"   ‚Ä¢ Joblib: {'‚úÖ' if JOBLIB_AVAILABLE else '‚ùå'}")
        print(f"   ‚Ä¢ Vectorized Operations: ‚úÖ")
        print(f"   ‚Ä¢ Parquet Caching: ‚úÖ")
        
        # Check for cached team histories
        cached_histories = self._load_cache_parquet('team_histories.parquet')
        
        if cached_histories is not None:
            print("üìÇ Using cached team histories")
            histories = []
            for _, row in cached_histories.iterrows():
                # Reconstruct team history from cached data
                history = {
                    'Team': row['Team'],
                    'Games_Played': row['Games_Played'],
                    'Total_Games': row['Total_Games'],
                    'Wins': row['Wins'],
                    'Losses': row['Losses'],
                    'Ties': row['Ties'],
                    'Goals_For': row['Goals_For'],
                    'Goals_Against': row['Goals_Against'],
                    'Goal_Diff': row['Goal_Diff'],
                    'Win_Pct': row['Win_Pct'],
                    'Games': pd.DataFrame()  # Games not cached for memory efficiency
                }
                histories.append(history)
        else:
            # Build team histories with vectorized operations
            histories = self.build_team_game_histories_with_matching()
            
            # Cache the results
            if histories:
                history_data = []
                for history in histories:
                    history_data.append({
                        'Team': history['Team'],
                        'Games_Played': history['Games_Played'],
                        'Total_Games': history['Total_Games'],
                        'Wins': history['Wins'],
                        'Losses': history['Losses'],
                        'Ties': history['Ties'],
                        'Goals_For': history['Goals_For'],
                        'Goals_Against': history['Goals_Against'],
                        'Goal_Diff': history['Goal_Diff'],
                        'Win_Pct': history['Win_Pct']
                    })
                
                history_df = pd.DataFrame(history_data)
                self._save_cache_parquet(history_df, 'team_histories.parquet')
        
        print(f"‚è± Team History Building took: {time.time() - t1:.1f}s")
        
        if not histories:
            print("‚ùå No team histories built")
            return None
        
        # Convert to DataFrame for ranking
        t2 = time.time()
        
        # Phase 4 Step 4: Use DuckDB for fast aggregations
        if histories and DUCKDB_AVAILABLE:
            print("üîÑ Using DuckDB for fast team stats aggregation...")
            # Convert histories to DataFrame for DuckDB processing
            history_data = []
            for history in histories:
                history_data.append({
                    'Team': history['Team'],
                    'Games_Played': history['Games_Played'],
                    'Total_Games': history['Total_Games'],
                    'Wins': history['Wins'],
                    'Losses': history['Losses'],
                    'Ties': history['Ties'],
                    'Goals_For': history['Goals_For'],
                    'Goals_Against': history['Goals_Against'],
                    'Goal_Diff': history['Goal_Diff'],
                    'Win_Pct': history['Win_Pct']
                })
            
            team_stats_df = pd.DataFrame(history_data)
        else:
            # Fallback to pandas processing
            team_stats = []
            for history in histories:
                team_stats.append({
                    'Team': history['Team'],
                    'Games_Played': history['Games_Played'],
                    'Total_Games': history['Total_Games'],
                    'Wins': history['Wins'],
                    'Losses': history['Losses'],
                    'Ties': history['Ties'],
                    'Goals_For': history['Goals_For'],
                    'Goals_Against': history['Goals_Against'],
                    'Goal_Diff': history['Goal_Diff'],
                    'Win_Pct': history['Win_Pct']
                })
            
            team_stats_df = pd.DataFrame(team_stats)
        
        # Sort by win percentage and goal difference
        team_stats_df = team_stats_df.sort_values(['Win_Pct', 'Goal_Diff'], ascending=[False, False])
        
        # Add ranking
        team_stats_df['Rank'] = range(1, len(team_stats_df) + 1)
        print(f"‚è± DataFrame Processing took: {time.time() - t2:.1f}s")
        
        print(f"‚úÖ Generated rankings for {len(team_stats_df)} teams")
        
        # Flush new aliases to persistent storage
        t3 = time.time()
        print("üîÑ Saving new aliases to persistent storage...")
        self.resolver.flush()
        print(f"‚è± Alias Flush took: {time.time() - t3:.1f}s")
        
        # Phase 4 Final Summary
        total_time = time.time() - total_start_time
        print(f"\nüéØ PHASE 4 PERFORMANCE SUMMARY:")
        print(f"‚è± TOTAL RUNTIME: {total_time:.1f}s ({total_time/60:.1f} minutes)")
        print(f"üìä Teams Processed: {len(team_stats_df)}")
        print(f"‚ö° Performance Optimizations:")
        print(f"   ‚Ä¢ Vectorized Alias Resolution: ‚úÖ")
        print(f"   ‚Ä¢ DuckDB Aggregations: {'‚úÖ' if DUCKDB_AVAILABLE else '‚ùå'}")
        print(f"   ‚Ä¢ Parallel SOS Calculation: {'‚úÖ' if JOBLIB_AVAILABLE else '‚ùå'}")
        print(f"   ‚Ä¢ Parquet Caching: ‚úÖ")
        print(f"   ‚Ä¢ Batch I/O Operations: ‚úÖ")
        
        if total_time < 1800:  # Less than 30 minutes
            print(f"üöÄ SUCCESS: Achieved target performance (< 30 minutes)!")
        else:
            print(f"‚ö†Ô∏è Still above target (30 min), but significant improvement from 4+ hours")
        
        return team_stats_df

def main():
    """Main function"""
    print("=== EFFICIENT U10 RANKINGS GENERATOR WITH ALIAS SYSTEM ===")
    
    generator = EfficientU10RankingsGenerator()
    rankings = generator.generate_rankings()
    
    if rankings is not None:
        # Save rankings
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        output_file = f'data/output/National_U10_Rankings_ALIAS_v{timestamp}.csv'
        
        rankings.to_csv(output_file, index=False)
        print(f"‚úÖ Rankings saved to: {output_file}")
        
        # Show top 10
        print("\n=== TOP 10 U10 TEAMS ===")
        print(rankings.head(10)[['Rank', 'Team', 'Games_Played', 'Total_Games', 'Win_Pct', 'Goal_Diff']].to_string(index=False))
        
        return rankings
    else:
        print("‚ùå Failed to generate rankings")
        return None

if __name__ == "__main__":
    rankings = main()
